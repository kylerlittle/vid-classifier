{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vimeo Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I attempt to classify staff-picked feature videos from Vimeo into \"similar\" groups. The goal of this script is to accept a clip_id (a unique identifier given to every video) as input and to output \"similar\" clips using my best judgement after exploring the dataset. Currently, the term \"similar\" is not yet defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll import the basic libraries I'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0: What does the data look like?\n",
    "Import the data set using pandas into a DataFrame object and ask it some questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_df = pd.read_csv(\"similar-staff-picks-challenge/similar-staff-picks-challenge-clips.csv\")\n",
    "print(\"Data shape: \" + str(video_df.shape))\n",
    "print(\"\\nInfo:\")\n",
    "print(video_df.info())\n",
    "print(\"\\nMissing Entries:\")\n",
    "print(video_df.isnull().sum())\n",
    "print(\"\\nStructure:\")\n",
    "print(video_df.head(3))   # first 3 rows\n",
    "fig = plt.figure()\n",
    "video_df.hist(column='duration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Processing\n",
    "In this stage, I need to \"clean up\" the data. This involves removing or accounting for null fields and\n",
    "removing invaluable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There are 105 missing entries in 'caption' field; need to fix this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Feature SelectionIn this stage, I determine which fields best classify the dataset and why. I ascertain other features from the given fields.\n",
    "frequencyâ€“inverse document frequency of \"description\" field\n",
    "---------> first need to clean this field up... there are null values & some of them just don't make any sense\n",
    "---------> https://towardsdatascience.com/how-i-used-machine-learning-to-classify-emails-and-turn-them-into-insights-efed37c1e66\n",
    "---------> http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "duration field\n",
    "total_comments field \n",
    "thumbnail field\n",
    "---------> extract a few features from the thumbnails such as: brightness, exposure, translation, rotation, scale, symmetry, intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def extract_im_features(URL):\n",
    "    response = requests.get(URL)\n",
    "    bytesIOobj = BytesIO( bytes(response.content))\n",
    "    bytesIOobj.seek(0)\n",
    "    byteIm = bytesIOobj.read()\n",
    "    img = Image.open(byteIm)\n",
    "    img.save(\"test.png\")\n",
    "    \n",
    "extract_im_features('https://i.vimeocdn.com/video/677861199_780x439.webp')\n",
    "\n",
    "\n",
    "\n",
    "# Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def extract_caption_tf_idf(caption_text):\n",
    "    '''\n",
    "    vect = TfidfVectorizer(stop_words='english', max_df=0.50, min_df=2)\n",
    "    X = vect.fit_transform(caption_text)\n",
    "    print(X)\n",
    "    return X\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(row):\n",
    "    # Initialize empty feature_vector as list\n",
    "    feature_vector = []\n",
    "\n",
    "    # Extract Features\n",
    "    extract_im_features(row['thumbnail'])\n",
    "    feature_vector.append(extract_caption_tf_idf(row['caption']))\n",
    "\n",
    "    # Return converted numpy array\n",
    "    return np.array(feature_vector)\n",
    "\n",
    "\n",
    "number_of_features = 10\n",
    "feature_matrix = np.zeros((video_df.shape[0], number_of_features))\n",
    "\n",
    "for index, row in video_df.iterrows():\n",
    "    feature_matrix[index] = vectorize(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Modeling\n",
    "In this stage, I determine which machine learning model is appropriate for the situation.\n",
    "\n",
    "For starters, it is clear I must use an unsupervised machine learning model to classify\n",
    "the data. Although I am given the \"categories\" that each video is put in, videos in \n",
    "the same category aren't necessarily \"similar.\" Furthermore, I don't need a model that\n",
    "predicts output based on unknown input. I'm only using the input data to determine\n",
    "the relationship between the data points, exactly what unsupervising machine learning\n",
    "attempts to do.\n",
    "\n",
    "The best machine learning model to do such a job is clustering. Clustering\n",
    "attempts to segregate groups of data points with similar traits and put them\n",
    "into clusters. This perfectly aligns with the goal of the project.\n",
    "\n",
    "For a given input (from the supplied data set), the model should output its 10 closest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 100; max_iterations = 1000\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters, max_iter=max_iterations).fit(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Testing\n",
    "Output results for the following clip_ids:\n",
    "14434107, 249393804, 71964690, 78106175, 228236677, 11374425, 93951774,\n",
    "35616659, 112360862, 116368"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
